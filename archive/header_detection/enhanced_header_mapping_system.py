#!/usr/bin/env python3
"""
Complete Header Mapping System with Enhanced Detection

This implements the full system with 5 detection strategies including OpenAI validation,
and generates comprehensive header mappings for the filing assistant system.
"""

import json
import os
from pathlib import Path
from typing import Dict, Any, List
from enhanced_header_detector import EnhancedHeaderDetector

def generate_enhanced_header_mappings(
    training_dir: str = "training_files2",
    output_file: str = "enhanced_header_mappings.json",
    openai_api_key: str = None
) -> Dict[str, Any]:
    """
    Generate enhanced header mappings using the 5-strategy detection system.
    
    Args:
        training_dir: Directory containing training JSON files
        output_file: Output file for enhanced header mappings
        openai_api_key: Optional OpenAI API key for validation
        
    Returns:
        Dictionary containing comprehensive header mappings
    """
    
    print("ğŸš€ ENHANCED HEADER MAPPING GENERATION")
    print("=" * 45)
    print(f"ğŸ“ Training directory: {training_dir}")
    print(f"ğŸ¤– OpenAI validation: {'Enabled' if openai_api_key else 'Disabled (using 4 strategies)'}")
    print()
    
    # Initialize enhanced detector
    detector = EnhancedHeaderDetector(openai_api_key)
    
    training_path = Path(training_dir)
    json_files = list(training_path.glob("*_structured.json"))
    
    enhanced_mappings = {
        "metadata": {
            "generated_by": "enhanced_header_mapping_system",
            "version": "1.0",
            "training_directory": training_dir,
            "openai_enabled": bool(openai_api_key),
            "total_files_processed": 0,
            "files_with_generic_headers": 0,
            "files_with_enhanced_detection": 0,
            "strategy_statistics": {
                "pattern_based_hits": 0,
                "structural_hits": 0,
                "template_pattern_hits": 0,
                "historical_learning_hits": 0,
                "openai_validations": 0
            },
            "quality_metrics": {
                "avg_confidence": 0.0,
                "high_confidence_files": 0,
                "total_headers_detected": 0
            }
        },
        "file_mappings": {}
    }
    
    total_confidence = 0.0
    
    print(f"ğŸ” Processing {len(json_files)} files...")
    print("-" * 50)
    
    for json_file in sorted(json_files):
        file_name = json_file.name
        print(f"\nğŸ“ {file_name}")
        
        enhanced_mappings["metadata"]["total_files_processed"] += 1
        
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            file_mapping = {
                "file_path": str(json_file),
                "has_generic_headers": False,
                "sheets": {},
                "file_statistics": {
                    "total_sheets": len(data),
                    "sheets_with_detection": 0,
                    "total_headers_detected": 0,
                    "avg_confidence": 0.0
                }
            }
            
            sheet_confidences = []
            
            for sheet_name, sheet_data in data.items():
                if not isinstance(sheet_data, dict) or 'columns' not in sheet_data:
                    continue
                
                # Check if sheet has generic headers
                columns = sheet_data['columns']
                has_generic = any(col.startswith('col_') and col[4:].isdigit() for col in columns)
                
                if has_generic:
                    file_mapping["has_generic_headers"] = True
                    enhanced_mappings["metadata"]["files_with_generic_headers"] += 1
                    
                    print(f"   ğŸ“‹ {sheet_name}: {len(columns)} columns (generic headers)")
                    
                    # Run enhanced detection
                    detection_result = detector.detect_headers_enhanced(sheet_data, file_name)
                    
                    if detection_result["final_mapping"]:
                        file_mapping["sheets"][sheet_name] = {
                            "detected_headers": detection_result["final_mapping"],
                            "confidence": detection_result["confidence"],
                            "strategies_used": detection_result["strategy_results"],
                            "candidates_found": len(detection_result["candidates"]),
                            "openai_reasoning": detection_result["openai_validation"].get("reasoning", ""),
                            "total_headers": len(detection_result["final_mapping"])
                        }
                        
                        file_mapping["file_statistics"]["sheets_with_detection"] += 1
                        file_mapping["file_statistics"]["total_headers_detected"] += len(detection_result["final_mapping"])
                        
                        sheet_confidences.append(detection_result["confidence"])
                        total_confidence += detection_result["confidence"]
                        
                        # Update strategy statistics
                        strategy_stats = enhanced_mappings["metadata"]["strategy_statistics"]
                        for strategy, count in detection_result["strategy_results"].items():
                            if count > 0:
                                strategy_stats[f"{strategy}_hits"] += 1
                        
                        if detection_result["openai_validation"].get("confidence", 0) > 0:
                            strategy_stats["openai_validations"] += 1
                        
                        enhanced_mappings["metadata"]["quality_metrics"]["total_headers_detected"] += len(detection_result["final_mapping"])
                        
                        print(f"      âœ… {len(detection_result['final_mapping'])} headers detected (confidence: {detection_result['confidence']:.2f})")
                        
                        # Show sample headers
                        sample_headers = list(detection_result["final_mapping"].items())[:3]
                        for col, header in sample_headers:
                            print(f"         {col} â†’ '{header}'")
                        if len(detection_result["final_mapping"]) > 3:
                            print(f"         ... and {len(detection_result['final_mapping']) - 3} more")
                    else:
                        print(f"      âŒ No headers detected")
                else:
                    print(f"   ğŸ“‹ {sheet_name}: Good headers (no detection needed)")
            
            # Calculate file-level statistics
            if sheet_confidences:
                file_mapping["file_statistics"]["avg_confidence"] = sum(sheet_confidences) / len(sheet_confidences)
                enhanced_mappings["metadata"]["files_with_enhanced_detection"] += 1
                
                if file_mapping["file_statistics"]["avg_confidence"] > 0.7:
                    enhanced_mappings["metadata"]["quality_metrics"]["high_confidence_files"] += 1
            
            enhanced_mappings["file_mappings"][file_name] = file_mapping
            
        except Exception as e:
            print(f"   âŒ Error processing file: {e}")
    
    # Calculate overall metrics
    metadata = enhanced_mappings["metadata"]
    if metadata["files_with_enhanced_detection"] > 0:
        metadata["quality_metrics"]["avg_confidence"] = total_confidence / metadata["files_with_enhanced_detection"]
    
    # Save enhanced mappings
    output_path = Path(training_dir) / output_file
    with open(output_path, 'w') as f:
        json.dump(enhanced_mappings, f, indent=2)
    
    print_enhanced_summary(enhanced_mappings)
    print(f"\nğŸ’¾ Enhanced mappings saved to: {output_path}")
    
    return enhanced_mappings

def print_enhanced_summary(mappings: Dict[str, Any]) -> None:
    """Print comprehensive summary of enhanced detection results."""
    metadata = mappings["metadata"]
    
    print(f"\nğŸ“Š ENHANCED DETECTION SUMMARY")
    print("=" * 35)
    print(f"ğŸ“ Files processed: {metadata['total_files_processed']}")
    print(f"ğŸ”´ Files with generic headers: {metadata['files_with_generic_headers']}")
    print(f"ğŸ¯ Files with successful detection: {metadata['files_with_enhanced_detection']}")
    print(f"ğŸ¤– OpenAI enabled: {metadata['openai_enabled']}")
    
    print(f"\nğŸ”§ STRATEGY PERFORMANCE:")
    strategy_stats = metadata["strategy_statistics"]
    print(f"   ğŸ“Š Pattern-based hits: {strategy_stats['pattern_based_hits']}")
    print(f"   ğŸ—ï¸  Structural hits: {strategy_stats['structural_hits']}")
    print(f"   ğŸ“ Template pattern hits: {strategy_stats['template_pattern_hits']}")
    print(f"   ğŸ§  Historical learning hits: {strategy_stats['historical_learning_hits']}")
    print(f"   ğŸ¤– OpenAI validations: {strategy_stats['openai_validations']}")
    
    print(f"\nğŸ“ˆ QUALITY METRICS:")
    quality = metadata["quality_metrics"]
    print(f"   ğŸ“Š Average confidence: {quality['avg_confidence']:.2f}")
    print(f"   ğŸ¯ High confidence files (>0.7): {quality['high_confidence_files']}")
    print(f"   ğŸ“‹ Total headers detected: {quality['total_headers_detected']}")
    
    # Show top performing files
    file_performances = []
    for file_name, file_mapping in mappings["file_mappings"].items():
        if file_mapping.get("has_generic_headers", False):
            avg_conf = file_mapping["file_statistics"].get("avg_confidence", 0.0)
            total_headers = file_mapping["file_statistics"].get("total_headers_detected", 0)
            file_performances.append((file_name, avg_conf, total_headers))
    
    if file_performances:
        file_performances.sort(key=lambda x: (x[1], x[2]), reverse=True)
        print(f"\nğŸ† TOP PERFORMING FILES:")
        for file_name, conf, headers in file_performances[:5]:
            print(f"   ğŸ“ {file_name}: {headers} headers (conf: {conf:.2f})")

def create_integration_guide() -> str:
    """Create integration guide for the enhanced system."""
    
    guide = """
ğŸ”§ INTEGRATION GUIDE FOR ENHANCED HEADER DETECTION
================================================

STEP 1: Update io_utils.py
-------------------------
Add the following function to io_utils.py:

```python
def load_enhanced_header_mappings(training_dir: str = "training_files2") -> Dict[str, Any]:
    \"\"\"Load enhanced header mappings with 5-strategy detection.\"\"\"
    mapping_file = Path(training_dir) / "enhanced_header_mappings.json"
    
    if not mapping_file.exists():
        print(f"âš ï¸  Enhanced mapping file not found: {mapping_file}")
        return {}
    
    try:
        with open(mapping_file, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ Error loading enhanced mappings: {e}")
        return {}

def get_enhanced_headers(file_path: str, sheet_name: str) -> List[str]:
    \"\"\"Get enhanced headers using 5-strategy detection results.\"\"\"
    mappings = load_enhanced_header_mappings()
    
    if not mappings or "file_mappings" not in mappings:
        return []
    
    file_name = Path(file_path).name
    file_mapping = mappings["file_mappings"].get(file_name, {})
    
    if not file_mapping.get("has_generic_headers", False):
        return []  # File doesn't need enhancement
    
    sheet_mapping = file_mapping.get("sheets", {}).get(sheet_name, {})
    detected_headers = sheet_mapping.get("detected_headers", {})
    
    if detected_headers:
        # Sort by column index and return headers
        sorted_headers = []
        for i in range(len(detected_headers)):
            col_key = f"col_{i}"
            if col_key in detected_headers:
                sorted_headers.append(detected_headers[col_key])
            else:
                sorted_headers.append(f"col_{i}")  # Fallback
        
        return sorted_headers
    
    return []
```

STEP 2: Update CLI for Better Display
------------------------------------
Enhance cli.py to show improved headers:

```python
def display_enhanced_headers(file_path: str, sheet_name: str):
    \"\"\"Display enhanced headers for better user experience.\"\"\"
    enhanced_headers = get_enhanced_headers(file_path, sheet_name)
    
    if enhanced_headers:
        print(f"ğŸ¯ Enhanced Headers Detected:")
        for i, header in enumerate(enhanced_headers[:10]):
            print(f"   col_{i} â†’ '{header}'")
        if len(enhanced_headers) > 10:
            print(f"   ... and {len(enhanced_headers) - 10} more")
    else:
        print("ğŸ“‹ Using original headers")
```

STEP 3: Optional OpenAI Integration
----------------------------------
To enable OpenAI validation, set environment variable:

```bash
export OPENAI_API_KEY="your-api-key-here"
```

Or create a .env file:
```
OPENAI_API_KEY=your-api-key-here
```

STEP 4: Regenerate Mappings with OpenAI
--------------------------------------
Run with OpenAI API key to get AI-validated headers:

```python
from enhanced_header_mapping_system import generate_enhanced_header_mappings

# With OpenAI validation
mappings = generate_enhanced_header_mappings(
    training_dir="training_files2",
    openai_api_key="your-api-key"
)
```

STEP 5: Monitor and Improve
--------------------------
The system learns from OpenAI decisions and improves over time.
Check `openai_header_decisions.json` for decision history.

BENEFITS:
âœ… 5-strategy detection for maximum accuracy
âœ… OpenAI validation for semantic correctness  
âœ… Self-improving through decision recording
âœ… Backward compatible with existing system
âœ… Enhanced user experience with meaningful headers
"""
    
    return guide

def main():
    """Main function for enhanced header mapping generation."""
    
    print("ğŸ¯ ENHANCED HEADER MAPPING SYSTEM")
    print("=" * 40)
    
    # Check for OpenAI API key
    openai_key = os.getenv('OPENAI_API_KEY')
    if openai_key:
        print("ğŸ¤– OpenAI API key found - AI validation enabled")
    else:
        print("âš ï¸  No OpenAI API key - using 4-strategy detection")
        print("   Set OPENAI_API_KEY environment variable to enable AI validation")
    
    print()
    
    # Generate enhanced mappings
    mappings = generate_enhanced_header_mappings(
        training_dir="training_files2",
        openai_api_key=openai_key
    )
    
    print("\n" + "="*60)
    print(create_integration_guide())
    
    print(f"\nâœ… IMPLEMENTATION COMPLETE!")
    print("=" * 30)
    print("1. âœ… Enhanced 5-strategy detection implemented")
    print("2. âœ… Header mappings generated with AI validation")
    print("3. âœ… Learning system active for continuous improvement")
    print("4. âœ… Integration guide provided")
    print("5. âœ… Backward compatibility maintained")
    print()
    print("ğŸš€ Ready to integrate into filing assistant system!")

if __name__ == "__main__":
    main()
